{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT Classification - Private Datasets - (Training)\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:**\n",
    "- Carlos Salgado - Github: [@socd06](https://github.com/socd06)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLACE-HOLDER-TEXT\n",
    "Suppose you run a deep learning company that provides NLP expertise. You have two clients: Bob and Alice. Each of them runs their own website were users can write reviews about movies that have watched.\n",
    "\n",
    "Bob and Alice have heard of the great services you provide and asked you to create a sentiment classifier to help them automatically assign a sentiment (positive or negative) to each users review.\n",
    "\n",
    "Now you think that this is a really good opportunity. If you pool data from both Bob's and Alice's datasets, you would be able to create a bigger dataset that you can use to train a better classifer.\n",
    "\n",
    "But... \n",
    "\n",
    "It turns out you are not allowed to do this; both datasets are private.\n",
    "\n",
    "You are informed that privacy regulations in both Bob's and Alice's countries, prevents them from revealing their data to any third party. You cannot move Bob's data to your company's machines. Same for Alice's. Each dataset is contrained to live on its owner's machine, and they cannot be mixed together to create a bigger dataset.\n",
    "\n",
    "Now you think about OpenMined, and their great library called PySyft that provides the possiblity to perform Federated Learning and Encrypted Computations. Without that, you will be able to train a single model on both datasets at the same time. and YOUR ARE RIGHT!\n",
    "\n",
    "However, ...\n",
    "\n",
    "As you new, text datasets cannot be consumed directly for training a neural training. You need to create numerical representations of each text review before PySyft can consume it. Text needs to be tokenized first, preprocessed and vector embedding should be used instead of plaintext. But how to do such preprocessing if you are not allowed to have access to plaintext data? \n",
    "\n",
    "**SyferText** can help you! With SyferText, you can define preprocessing components that you can send over a network to Bob's and Alice's machine to perform preprocessing remotely, blindly and in a completely secure fashion. SyferText components do all the work from processing plaintext to obtaining its vector representation and encrypting it to hand it over to PySyft models for training. All without you accessing the data, and without the data quitting their owners machine.\n",
    "\n",
    "If you are wondering how that works, keep on following this tutorial.\n",
    "\n",
    "\n",
    "**Let's summarize:**\n",
    "\n",
    "1. You need to create a bigger dataset out of Bob's and Alice's smaller datasets. *(PySyft has the arsenal for that)*\n",
    "\n",
    "2. You need to prepare and preprocess the text data on Bob's and Alice's machines without revealing it, without moving any datasets to your machine, and without the need to work directly on Bob's or Alice's machines. *(SyferText to the rescue)*\n",
    "\n",
    "For this tutorial, we are going to work with the IMDB movie review dataset. This is a public dataset. But we are going to break it into two part, send each part to a differet PySyft work. We  consider that each part is a private dataset owned by the PySyft worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Importing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first install and import some libraries that we are going to be used all along this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements'\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "# requirements should include syfertext, sklearn, numpy, matplotlib, seaborn\n",
    "# I was missing sklearn, matplotlib, seaborn in my venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.2.1-cp37-cp37m-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /home/carlos/anaconda3/envs/pysyft/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1\n",
      "  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: numpy>=1.11 in /home/carlos/anaconda3/envs/pysyft/lib/python3.7/site-packages (from matplotlib) (1.18.2)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.2.0-cp37-cp37m-manylinux1_x86_64.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 1.9 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/carlos/anaconda3/envs/pysyft/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib) (1.14.0)\n",
      "Installing collected packages: cycler, pyparsing, kiwisolver, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.2.0 matplotlib-3.2.1 pyparsing-2.4.7\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.10.0-py3-none-any.whl (215 kB)\n",
      "\u001b[K     |████████████████████████████████| 215 kB 789 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /home/carlos/anaconda3/envs/pysyft/lib/python3.7/site-packages (from seaborn) (1.18.2)\n",
      "Collecting pandas>=0.22.0\n",
      "  Downloading pandas-1.0.3-cp37-cp37m-manylinux1_x86_64.whl (10.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.0 MB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=2.1.2 in /home/carlos/anaconda3/envs/pysyft/lib/python3.7/site-packages (from seaborn) (3.2.1)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /home/carlos/anaconda3/envs/pysyft/lib/python3.7/site-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/carlos/anaconda3/envs/pysyft/lib/python3.7/site-packages (from pandas>=0.22.0->seaborn) (2.8.1)\n",
      "Collecting pytz>=2017.2\n",
      "  Using cached pytz-2019.3-py2.py3-none-any.whl (509 kB)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/carlos/anaconda3/envs/pysyft/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/carlos/anaconda3/envs/pysyft/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/carlos/anaconda3/envs/pysyft/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/carlos/anaconda3/envs/pysyft/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.22.0->seaborn) (1.14.0)\n",
      "Installing collected packages: pytz, pandas, seaborn\n",
      "Successfully installed pandas-1.0.3 pytz-2019.3 seaborn-0.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SyferText imports\n",
    "import syfertext\n",
    "from syfertext.pipeline import SimpleTagger\n",
    "\n",
    "# Import useful utility functions for this tutorial\n",
    "from utils import download_dataset\n",
    "\n",
    "# PySyft and PyTorch import\n",
    "import syft as sy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "# Useful imports\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "sb.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Download the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <p style='color:red;'> (IGNORE THIS STEP IF YOU HAVE ALREADY DONE IT) </p>\n",
    "</div>\n",
    "\n",
    "# PLACEHOLDER-TEXT\n",
    "\n",
    "The dataset will be downloaded in a folder called `./imdb` in the same directory as the current notebook's. Four files are going to be downloaded:\n",
    "\n",
    "- `imdb.csv`: This is the dataset file containing 50K labeled reviews. It is a csv file composed of two columns: `review` and `sentiment`. The `review` column holds the review's text, and the `sentiment` column has one of two values: 'positive' or 'negative' to describe the overall sentiment of the review.\n",
    "- `stop_word_en.txt`: This is just a text file with a list of stop words according to NLTK.\n",
    "- `imdb_vocab.txt`: a list of all vocabulary of the dataset. One word per line.\n",
    "- `imdb_polarity.txt`: It hold the polarity value of each word in `imdb_vocab.txt`. A word that appears more often in positive reviews will have a higher polarity value than one that more frequently encountered in negative reviews.\n",
    "\n",
    "It is important to note that only the dataset `imdb.csv` is considered private. All other files in the above list are not under any privacy constraints.\n",
    "\n",
    "Please run the below cell in order to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "mtsamples:   0%|          | 0.00/17.0M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to download dataset: `mtsamples` ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "mtsamples:   0%|          | 1.02k/17.0M [00:00<37:20, 7.59kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:   0%|          | 82.9k/17.0M [00:00<26:07, 10.8kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:   2%|▏         | 263k/17.0M [00:00<18:08, 15.4kB/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:   3%|▎         | 460k/17.0M [00:00<12:35, 21.9kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:   4%|▍         | 656k/17.0M [00:00<08:45, 31.1kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:   5%|▍         | 798k/17.0M [00:00<06:08, 44.0kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:   5%|▌         | 935k/17.0M [00:00<04:19, 62.0kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:   6%|▋         | 1.08M/17.0M [00:00<03:03, 87.0kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:   7%|▋         | 1.25M/17.0M [00:00<02:09, 121kB/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:   8%|▊         | 1.43M/17.0M [00:01<01:32, 168kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:   9%|▉         | 1.61M/17.0M [00:01<01:06, 230kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  11%|█         | 1.82M/17.0M [00:01<00:48, 313kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  12%|█▏        | 2.03M/17.0M [00:01<00:35, 420kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  13%|█▎        | 2.23M/17.0M [00:01<00:27, 547kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  14%|█▍        | 2.44M/17.0M [00:01<00:20, 703kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  15%|█▌        | 2.63M/17.0M [00:01<00:16, 865kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  17%|█▋        | 2.82M/17.0M [00:01<00:15, 891kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  18%|█▊        | 2.98M/17.0M [00:02<00:14, 974kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  19%|█▊        | 3.15M/17.0M [00:02<00:12, 1.09MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  19%|█▉        | 3.30M/17.0M [00:02<00:11, 1.14MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  20%|██        | 3.46M/17.0M [00:02<00:10, 1.24MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  21%|██▏       | 3.64M/17.0M [00:02<00:09, 1.35MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  22%|██▏       | 3.79M/17.0M [00:02<00:09, 1.33MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  23%|██▎       | 3.97M/17.0M [00:02<00:09, 1.42MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  24%|██▍       | 4.12M/17.0M [00:02<00:09, 1.41MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  25%|██▌       | 4.27M/17.0M [00:02<00:09, 1.35MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  26%|██▌       | 4.41M/17.0M [00:03<00:09, 1.34MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  27%|██▋       | 4.57M/17.0M [00:03<00:08, 1.41MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  28%|██▊       | 4.72M/17.0M [00:03<00:08, 1.41MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  29%|██▊       | 4.86M/17.0M [00:03<00:08, 1.40MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  29%|██▉       | 5.00M/17.0M [00:03<00:08, 1.35MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  30%|███       | 5.14M/17.0M [00:03<00:08, 1.34MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  31%|███       | 5.28M/17.0M [00:03<00:09, 1.27MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  32%|███▏      | 5.41M/17.0M [00:03<00:09, 1.18MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  33%|███▎      | 5.53M/17.0M [00:03<00:10, 1.14MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  33%|███▎      | 5.65M/17.0M [00:04<00:09, 1.15MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  34%|███▍      | 5.78M/17.0M [00:04<00:09, 1.19MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  35%|███▍      | 5.91M/17.0M [00:04<00:09, 1.18MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  36%|███▌      | 6.06M/17.0M [00:04<00:08, 1.25MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  37%|███▋      | 6.23M/17.0M [00:04<00:08, 1.34MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  38%|███▊      | 6.39M/17.0M [00:04<00:08, 1.26MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  38%|███▊      | 6.55M/17.0M [00:04<00:07, 1.34MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  39%|███▉      | 6.68M/17.0M [00:04<00:07, 1.33MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  40%|████      | 6.82M/17.0M [00:04<00:08, 1.23MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  41%|████      | 6.94M/17.0M [00:05<00:08, 1.24MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  42%|████▏     | 7.11M/17.0M [00:05<00:07, 1.32MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  43%|████▎     | 7.26M/17.0M [00:05<00:07, 1.34MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  43%|████▎     | 7.40M/17.0M [00:05<00:07, 1.24MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  44%|████▍     | 7.52M/17.0M [00:05<00:08, 1.13MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  45%|████▍     | 7.64M/17.0M [00:05<00:08, 1.07MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  46%|████▌     | 7.75M/17.0M [00:05<00:09, 976kB/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  46%|████▌     | 7.87M/17.0M [00:05<00:09, 945kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  47%|████▋     | 7.98M/17.0M [00:06<00:09, 978kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  48%|████▊     | 8.11M/17.0M [00:06<00:08, 1.05MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  48%|████▊     | 8.24M/17.0M [00:06<00:08, 1.09MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  49%|████▉     | 8.37M/17.0M [00:06<00:07, 1.12MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  50%|████▉     | 8.49M/17.0M [00:06<00:08, 980kB/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  51%|█████     | 8.59M/17.0M [00:06<00:09, 903kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  51%|█████     | 8.70M/17.0M [00:06<00:08, 949kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  52%|█████▏    | 8.83M/17.0M [00:06<00:07, 1.03MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  53%|█████▎    | 8.96M/17.0M [00:06<00:07, 1.09MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  53%|█████▎    | 9.08M/17.0M [00:07<00:07, 1.06MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  54%|█████▍    | 9.19M/17.0M [00:07<00:07, 1.01MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  55%|█████▍    | 9.30M/17.0M [00:07<00:07, 1.00MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  55%|█████▌    | 9.42M/17.0M [00:07<00:07, 1.06MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  56%|█████▌    | 9.55M/17.0M [00:07<00:06, 1.10MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  57%|█████▋    | 9.67M/17.0M [00:07<00:06, 1.08MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  58%|█████▊    | 9.81M/17.0M [00:07<00:06, 1.17MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  59%|█████▊    | 9.98M/17.0M [00:07<00:05, 1.25MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  60%|█████▉    | 10.1M/17.0M [00:07<00:05, 1.26MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  61%|██████    | 10.3M/17.0M [00:08<00:05, 1.33MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  62%|██████▏   | 10.5M/17.0M [00:08<00:04, 1.47MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  63%|██████▎   | 10.7M/17.0M [00:08<00:04, 1.57MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  64%|██████▍   | 10.9M/17.0M [00:08<00:03, 1.68MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  65%|██████▌   | 11.1M/17.0M [00:08<00:03, 1.77MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  66%|██████▋   | 11.3M/17.0M [00:08<00:03, 1.75MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  68%|██████▊   | 11.5M/17.0M [00:08<00:03, 1.70MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  69%|██████▊   | 11.7M/17.0M [00:08<00:03, 1.71MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  70%|██████▉   | 11.8M/17.0M [00:08<00:03, 1.59MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  71%|███████   | 12.0M/17.0M [00:09<00:03, 1.63MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  72%|███████▏  | 12.2M/17.0M [00:09<00:02, 1.71MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  73%|███████▎  | 12.4M/17.0M [00:09<00:02, 1.78MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  74%|███████▍  | 12.6M/17.0M [00:09<00:02, 1.76MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  75%|███████▌  | 12.8M/17.0M [00:09<00:02, 1.68MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  76%|███████▌  | 12.9M/17.0M [00:09<00:02, 1.61MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  77%|███████▋  | 13.1M/17.0M [00:09<00:02, 1.33MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  78%|███████▊  | 13.3M/17.0M [00:09<00:02, 1.32MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  79%|███████▉  | 13.4M/17.0M [00:09<00:02, 1.39MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  80%|███████▉  | 13.6M/17.0M [00:10<00:02, 1.41MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  81%|████████  | 13.7M/17.0M [00:10<00:02, 1.42MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  82%|████████▏ | 13.9M/17.0M [00:10<00:02, 1.50MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  83%|████████▎ | 14.1M/17.0M [00:10<00:01, 1.57MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  84%|████████▍ | 14.3M/17.0M [00:10<00:01, 1.70MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  85%|████████▌ | 14.5M/17.0M [00:10<00:01, 1.76MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  87%|████████▋ | 14.7M/17.0M [00:10<00:01, 1.72MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  88%|████████▊ | 15.0M/17.0M [00:10<00:01, 1.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  89%|████████▉ | 15.2M/17.0M [00:10<00:00, 1.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  90%|█████████ | 15.4M/17.0M [00:11<00:00, 1.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  92%|█████████▏| 15.6M/17.0M [00:11<00:00, 1.53MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  93%|█████████▎| 15.7M/17.0M [00:11<00:00, 1.38MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  93%|█████████▎| 15.9M/17.0M [00:11<00:00, 1.15MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  94%|█████████▍| 16.0M/17.0M [00:11<00:00, 1.01MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  95%|█████████▍| 16.1M/17.0M [00:11<00:00, 885kB/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  95%|█████████▌| 16.2M/17.0M [00:11<00:00, 918kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  96%|█████████▌| 16.3M/17.0M [00:12<00:00, 923kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  97%|█████████▋| 16.4M/17.0M [00:12<00:00, 886kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  97%|█████████▋| 16.6M/17.0M [00:12<00:00, 962kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  98%|█████████▊| 16.7M/17.0M [00:12<00:00, 938kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples:  99%|█████████▊| 16.8M/17.0M [00:12<00:00, 727kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mtsamples:  99%|█████████▉| 16.9M/17.0M [00:12<00:00, 667kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mtsamples: 17.0MB [00:13, 1.31MB/s]                           \u001b[A\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# The URL template to all dataset files\n",
    "url_template = 'https://raw.githubusercontent.com/socd06/medical_transcriptions/master/%s'\n",
    "\n",
    "# File names to be downloaded from the using the URL template above\n",
    "files = ['mtsamples.csv']\n",
    "\n",
    "# Construct the list of urls\n",
    "urls = [url_template % file for file in files]\n",
    "\n",
    "# The dataset name and its root folder\n",
    "dataset_name = 'mtsamples'\n",
    "root_path = './mtsamples'\n",
    "\n",
    "# Create the dataset folder if it is not already there\n",
    "if not os.path.exists('./mtsamples'):\n",
    "    os.mkdir('./mtsamples')\n",
    "\n",
    "# Start downloading\n",
    "download_dataset(dataset_name = dataset_name, \n",
    "                 urls = urls, \n",
    "                 root_path = root_path\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Preparing the work environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLACEHOLDER-TEXT \n",
    "As I explained in the introduction, we will simulate a work environment with three main actors, a company (me) and two clients owning two private datasets (Bob and Alice). In PySyft terminology, this translates to creating a worker to represent each actor. We will also need a fourth worker, the crypto provider, which provides the primitives for using Secure Multi-Party Encryption (SMPC) that we will apply to encrypt word embeddings and the model itself before training. Here is how we do it with PySyft:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "# Create a torch hook for PySyft\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "# Create some PySyft workers\n",
    "me = hook.local_worker # This is the worker representing the deep learning company\n",
    "bob = sy.VirtualWorker(hook, id = 'bob') # Bob owns the first dataset\n",
    "alice = sy.VirtualWorker(hook, id = 'alice') # Alice owns the second dataset\n",
    "\n",
    "crypto_provider = sy.VirtualWorker(hook, id = 'crypto_provider') # provides encryption primitive for SMPC\n",
    "\n",
    "# Create a summary writer for logging performance with Tensorboard\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Simulating Private Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLACEHOLDER-TEXT\n",
    "In order to simulate have two private datasets owned by two different clients, Bob and Alice. We will do the following:\n",
    "\n",
    "1. Load the whole dataset in `imdb.csv` locally (the `me` worker). This dataset will be loaded as a list of dictionaries that has the following format: `[ {'review': <review text>, 'label': <1 or 0>}, {...}, {...}]`\n",
    "\n",
    "\n",
    "2. Split the dataset into two parts, one for Bob and the other for Alice. Each part will be also split into a training set and a validation set. This will create four lists: `train_bob`, `valid_bob`, `train_alice`, `valid_alice`. Each listhas the same format I mentioned above.\n",
    "\n",
    "\n",
    "3. Each element in the four lists will be sent to the corresponding worker. This will change the content of the lists as depicted in **Figure(1)**. Each list willl hold PySyft pointers to the texts and labels instead of the objects themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<br>\n",
    "<img alt = 'imdb review remote datasets' src ='./art/imdb_review_remote.png' style='width:700px'>\n",
    "<div>\n",
    "<div style='width:600px;margin:30px auto 10px auto;text-align:center;'>\n",
    "<strong> Figure(1): </strong> The reviews and their labels are remotely located on Bob's and Alice's remote machines, only pointers to them are kept by the local worker (the company's machine).\n",
    "</div>\n",
    "</div>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the dataset locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the dataset file\n",
    "dataset_path = './imdb/imdb.csv'\n",
    "\n",
    "# store the dataset as a list of dictionaries\n",
    "# each dictionary has two keys, 'review' and 'label'\n",
    "# the 'review' element is a PySyft String\n",
    "# the 'label' element is an integer with 1 for 'positive'\n",
    "# and 0 for 'negative' review\n",
    "dataset_local = []\n",
    "\n",
    "with open(dataset_path, 'r') as dataset_file:\n",
    "    \n",
    "    # Create a csv reader object\n",
    "    reader = csv.DictReader(dataset_file)\n",
    "    \n",
    "    for elem in reader:\n",
    "        \n",
    "        # Create one entry\n",
    "        example = dict(review = String(elem['review']),\n",
    "                       label = 1 if elem['sentiment'] == 'positive' else 0\n",
    "                      )\n",
    "        \n",
    "        # add to the local dataset\n",
    "        dataset_local.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how an element in the list looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': (Wrapper)>[PointerTensor | me:35928650540 -> bob:85041041421],\n",
      " 'review': 'Phil the Alien is one of those quirky films where the humour is based around the oddness of everything rather than actual punchlines.<br /><br />At first it was very odd and pretty funny but as the movie progressed I didn\\'t find the jokes or oddness funny anymore.<br /><br />Its a low budget film (thats never a problem in itself), there were some pretty interesting characters, but eventually I just lost interest.<br /><br />I imagine this film would appeal to a stoner who is currently partaking.<br /><br />For something similar but better try \"Brother from another planet\"'}\n"
     ]
    }
   ],
   "source": [
    "example = dataset_local[10]\n",
    "pprint(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'syft.generic.pointers.string_pointer.StringPointer'>\n"
     ]
    }
   ],
   "source": [
    "print(type(example['review']))\n",
    "print(type(example['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This review text is a PySyft `String` object. The label is an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the dataset into two equal parts and send each part to a different worker simulating two remote datasets as I mentioned above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two datasets, one for Bob, and the other for Alic\n",
    "dataset_bob, dataset_alice = train_test_split(dataset_local, train_size = 0.5)\n",
    "\n",
    "# Now create a validation set for Bob, and another for Alice\n",
    "train_bob, val_bob = train_test_split(dataset_bob, train_size = 0.7)\n",
    "train_alice, val_alice = train_test_split(dataset_alice, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now I will make the dataset remote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that sends the content of each split to a remote worker\n",
    "def make_remote_dataset(dataset, worker):\n",
    "\n",
    "    # Got through each example in the dataset\n",
    "    for example in dataset:\n",
    "        \n",
    "        # Send each review text\n",
    "        example['review'] = example['review'].send(worker)\n",
    "\n",
    "        # Send each label as a one-hot-enceded vector\n",
    "        one_hot_label = torch.zeros(2).scatter(0, torch.Tensor([example['label']]).long(), 1)\n",
    "        \n",
    "        # Send the review label\n",
    "        example['label'] = one_hot_label.send(worker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the above function, transforms the label to a one-hot-encoded format before sending it to a remote worker. So if the sentiment is negative, the corresponding tensor will hold `[1,0]`, and if it is positive, the label will be `[0,1]`. I actually choose this representation over a simple `1` or `0` due to some limitations in PySyft at the moment of writing this blogpost. This limitation is being worked on, and will be solved very soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can finally create the remote datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bob's remote dataset\n",
    "make_remote_dataset(train_bob, bob)\n",
    "make_remote_dataset(val_bob, bob)\n",
    "\n",
    "# Alice's remote dataset\n",
    "make_remote_dataset(train_alice, alice)\n",
    "make_remote_dataset(val_alice, alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me show you what an element of Bob's dataset look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'syft.generic.pointers.string_pointer.StringPointer'>\n",
      "(Wrapper)>[PointerTensor | me:17018428705 -> bob:81226937210]\n"
     ]
    }
   ],
   "source": [
    "# Take an element from the dataset\n",
    "example = train_bob[10]\n",
    "\n",
    "print(type(example['review']))\n",
    "print(example['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, the text type is now a PySyft `StringPointer` that points to the real `String` object  located in Bob's machine. The label type is a PySyft `PointerTensor`. Let's check out the location of the real text and label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<VirtualWorker id:bob #objects:50003>\n",
      "<VirtualWorker id:bob #objects:50003>\n"
     ]
    }
   ],
   "source": [
    "print(example['review'].location)\n",
    "print(example['label'].location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, you can see it for yourself, they are located in Bob's machine. This confirms **Figure(1)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are now ready, and so is the work environment. Let's start the fun with SyferText :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Create a `SyferText` Language object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Language object in SyferText is the master object. It orchestrates all the work done by SyferText. Let's create one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Language object with SyferText\n",
    "nlp = syfertext.load('en_core_web_lg', owner = me)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever you create a Language object as we did above, a pipeline will be created. At initialization, a pipeline only contains a tokenizer. You can see this for yourself using the `pipeline_template` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'remote': True, 'name': 'tokenizer'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the tokenizer entry has a propery called `remote` set to `True`. This means that we allow the tokenizer to be sent to a remote worker in case the string to be tokenized live there.\n",
    "\n",
    "We can add more components to the pipeline by using the `add_pipe` method of the Language class. One component we can add is a `SimpleTagger` object. This is a SyferText object that we can use to set custom attributes to individual tokens. In this tutorial, I will create two such  taggers: One that tags tokens that are stop words, the other tags each token as polar or not. \n",
    "\n",
    "By tagging a token, I mean setting a custom attribute to that token and assigning it a given value. For example, I set an attribute called `is_stop` with a value `True` for a stop word, and `False` otherwise.\n",
    "\n",
    "You can refer to **Figure(2)** to see how a pipeline is distributed on multiple workers on the dataset to preprocess is remote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Create a  tagger for stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by creating the stop-word tagger. Let's first load the stop word file into a list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of stop words\n",
    "with open('./imdb/stop_word_en.txt', 'r') as f:\n",
    "    stop_words = set(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the tagger which is an object of the `SimpleTagger` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple tagger object to tag stop words\n",
    "stop_tagger = SimpleTagger(attribute = 'is_stop',\n",
    "                           lookups = stop_words,\n",
    "                           tag = True,\n",
    "                           default_tag = False,\n",
    "                           case_sensitive = False\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that I pass the list of words as the `lookups` arguments. \n",
    "\n",
    "Every token in the `Doc` object will be given a custom attribute called `is_stop`. Every time a stop word is found, this attribute will be given the value `True` specified by the `tag` argument of the `SimpleTagger` class initialiser, otherwise, the `default_tag` will be used, i.e., `False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Create a tagger for most polar words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way we created a tagger for stop words. We are now going to create another tagger for polar words, i.e., words that are more biased toward a positive or a negative sentiment. Let's load the corresponding files `imdb_vocab.txt` and `imdb_polarity.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the polarity info\n",
    "with open('./imdb/imdb_vocab.txt', 'r') as f:\n",
    "    imdb_words = f.read().splitlines()\n",
    "    \n",
    "with open('./imdb/imdb_polarity.txt', 'r') as f:\n",
    "    polarity = [float(line) for line in f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me show you the distribution of polarity values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the histogram of polarity values\n",
    "fig, ax = plt.subplots(figsize = (10,5))\n",
    "sb.distplot(polarity, kde = False, ax = ax)\n",
    "\n",
    "ax.set_xlabel('Sentiment Polarity Value')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title(\"Distribution of Polarity Values in the IMDB dataset\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the grand majority of words seem to be unbiased toward a specific sentiment. So let's create a tagger that tags only tokens that are most polar by setting a custom attribute we will call `is_polar` to `True` and `False` otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose low/high polarity cutoff values\n",
    "low_cutoff = -0.5\n",
    "high_cutoff = 0.5\n",
    "\n",
    "# Create a list of polar tokens\n",
    "polar_tokens = [token for i, token in enumerate(imdb_words) \n",
    "                if polarity[i] > high_cutoff or\n",
    "                polarity[i] < low_cutoff]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the list of polar word above, we can now create the tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_tagger = SimpleTagger(attribute = 'is_polar',\n",
    "                               lookups = polar_tokens,\n",
    "                               tag = True,\n",
    "                               default_tag = False,\n",
    "                               case_sensitive = False\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Adding the taggers to the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add each tagger we created above by using the `add_pipe()` method of the `Language` class. However, in the following cell, I give you the possibility to decide for yourself which components you wish to add.\n",
    "\n",
    "Here is what I recommend you do:\n",
    "\n",
    "1. First run this tutorial without adding an tagger.\n",
    "\n",
    "2. Restart the notebook and run the tutorial again with `use_stop_tagger = True`.\n",
    "\n",
    "3. Restart the notebook and run the tutorial again with both `use_stop_tagger = True` and `use_polarity_tagger = True`.\n",
    "\n",
    "I will actually show you the results of each such run at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_stop_tagger = False\n",
    "use_polarity_tagger = False\n",
    "\n",
    "# Token with these custom tags\n",
    "# will be excluded from creating\n",
    "# the Doc vector\n",
    "excluded_tokens = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the above cell. I create a dictionary called `excluded_tokens`. It will be used later in this tutorial when we create embedding vectors for reviews. It enables us to execlude some tokens when we create a document embedding. Such exclusion will be based on the value of the custom attributes we set with the taggers.\n",
    "\n",
    "Now let's add the stop word tagger to the pipeline (If `use_stop_tagger = True`). Notice that I set the argument `remote = True`. This tells the `Language` object that it is allowed to send the pipe component to the remote worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_stop_tagger:\n",
    "\n",
    "    # Add the stop word to the pipeline\n",
    "    nlp.add_pipe(name = 'stop tagger',\n",
    "                 component = stop_tagger,\n",
    "                 remote = True\n",
    "                )\n",
    "\n",
    "    # Tokens with 'is_stop' = True are\n",
    "    # not going to be used when creating the \n",
    "    # Doc vector\n",
    "    excluded_tokens['is_stop'] = {True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for adding the polar word tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_polarity_tagger:\n",
    "    \n",
    "    # Add the polarity tagger to the pipeline\n",
    "    nlp.add_pipe(name = 'polarity tagger',\n",
    "                 component = polarity_tagger,\n",
    "                 remote = True\n",
    "                )\n",
    "\n",
    "    # Tokens with 'is_polar' = False are\n",
    "    # not going to be used when creating the \n",
    "    # Doc vector\n",
    "    excluded_tokens['is_polar'] = {False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out what pipe components are included in the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'remote': True, 'name': 'tokenizer'},\n",
       " {'remote': True, 'name': 'stop tagger'}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the remote datasets ready for use, and that SyferText's `Language` object set up with the appropriate pipeline, it's time to create data loaders that will take over the task of creating batches for training and validation.\n",
    "\n",
    "We will be using regular PyTorch data loaders to accomplish that. \n",
    "\n",
    "Each batch will be composed of a mix of training examples coming from both Bob's and Alice's datasets. Actually, for the data loader, there is only one big dataset, it is completely ignorant of the fact that data is distributed over different workers. \n",
    "\n",
    "Each example in the batch contains an encrypted version of one review's embedding vector and its encrypted label. For this tutorial, I compute such a vector as an average of the review's individual token vectors taken from the `en_core_web_lg` language model. Of course, all tokens with custom tags indicated in `excluded_tokens` won't be taken into account in computing a review's vector.\n",
    "\n",
    "If you look at **Figure(2)** you can see the big picture of how a single review text is remotely preprocessed by SyferText: \n",
    "\n",
    "1. First, the `Language` object `nlp` is used to preprocess one review on Bob's or Alice's machine.\n",
    "2. The object `nlp` determines that the real review text is actually remote, so it sends a subpipeline containing the required pipeline components we defined to the corresponding worker.\n",
    "3. The subpipeline is run and a `Doc` object is created on the remote worker containing the review's individual tokens appropriately tokenized and tagged.\n",
    "4. On the local worker, a `DocPointer` object is created pointing to that `Doc` object.\n",
    "5. By calling `get_encrypted_vector()` on the `DocPointer`, the call is forwarded to `Doc`, which, in turn, computes the `Doc` vector, encrypts it with SMPC using PySyft and returns it to the caller at the local worker.\n",
    "6. The PyTorch dataloader takes this encrypted vector and appends it to the training or validation batch.\n",
    "\n",
    "Notice that at no moment in the process, the plaintext data of the remote datasets are revealed to the local worker. *Privacy is preserved thanks to SyferText and PySyft!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<br>\n",
    "<img alt =  'SyferText pipeline' src ='./art/imdb_pipeline.png' style='width:700px;'>\n",
    "<div>\n",
    "<p style='width:600px;margin:30px auto 10px auto;text-align:center;'>\n",
    "<strong> Figure(2): </strong> A pipeline on the local worker only contains pointers to subpipelines carrying out the actual preprocessing on remote workers.\n",
    "</p>\n",
    "</div>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the steps described above, except for *step 6.* are carried out in the `__getitem__()` method of the custom PyTorch `Dataset` object that I define below. Please take a few minutes to check it out below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetIMDB(Dataset):\n",
    "    \n",
    "    def __init__(self, sets, share_workers, crypto_provider, nlp):\n",
    "        \"\"\"Initialize the Dataset object\n",
    "        \n",
    "        Args:\n",
    "            sets (list): A list containing all training OR \n",
    "                all validation sets to be used.\n",
    "            share_workers (list): T list of workers that will\n",
    "                be used to hold the SMPC shares.\n",
    "            crypto_provider (worker): A worker that will \n",
    "                provide SMPC primitives for encryption.\n",
    "            nlp: This is SyferText's Language object containing\n",
    "                the preprocessing pipeline.\n",
    "        \"\"\"\n",
    "        self.sets = sets\n",
    "        self.crypto_provider = crypto_provider\n",
    "        self.workers = share_workers\n",
    "    \n",
    "        # Create a single dataset unifying all datasets\n",
    "        # A property called `self.dataset` is created \n",
    "        # as a result of this call.\n",
    "        self._create_dataset()\n",
    "        \n",
    "        # The language model\n",
    "        self.nlp = nlp\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"In this function, preprocessing with SyferText \n",
    "        of one review will be triggered. Encryption will also\n",
    "        be performed and the encrypted vector will be obtained.\n",
    "        The encrypted label will be computed too.\n",
    "        \n",
    "        Args:\n",
    "            index (int): This is an integer received by the \n",
    "                PyTorch DataLoader. It specifies the index of\n",
    "                the example to be fetched. This actually indexes\n",
    "                one example in `self.dataset` which pools over\n",
    "                example of all the remote datasets.\n",
    "        \"\"\"\n",
    "        \n",
    "        # get the example\n",
    "        example = self.dataset[index]\n",
    "        \n",
    "        # Run the preprocessing pipeline on \n",
    "        # the review text and get a DocPointer object\n",
    "        doc_ptr = self.nlp(example['review'])\n",
    "        \n",
    "        # Get the encrypted vector embedding for the document\n",
    "        vector_enc = doc_ptr.get_encrypted_vector(bob, \n",
    "                                                  alice, \n",
    "                                                  crypto_provider = self.crypto_provider,\n",
    "                                                  requires_grad = True,\n",
    "                                                  excluded_tokens = excluded_tokens\n",
    "                                                 )\n",
    "        \n",
    "        # The only reason I am expecting an AttributeError\n",
    "        # here is this PySyft issue in version 0.2.4:\n",
    "        # https://github.com/OpenMined/PySyft/issues/3261\n",
    "        # Once the issue is fixed, there would be no need\n",
    "        # for the try/except statement here.\n",
    "        try:\n",
    "            \n",
    "            # Encrypte the target label\n",
    "            label_enc = example['label'].fix_precision().share(bob, \n",
    "                                                               alice, \n",
    "                                                               crypto_provider = self.crypto_provider,\n",
    "                                                               requires_grad = True\n",
    "                                                              ).get()\n",
    "\n",
    "        except AttributeError:\n",
    "            \n",
    "            label_enc = example['label'].share(bob, \n",
    "                                               alice, \n",
    "                                               crypto_provider = self.crypto_provider,\n",
    "                                               requires_grad = True\n",
    "                                              ).get()\n",
    "\n",
    "        return vector_enc, label_enc\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the combined size of all of the \n",
    "        remote training/validation sets.\n",
    "        \"\"\"\n",
    "        \n",
    "        # The size of the combined datasets\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def _create_dataset(self):\n",
    "        \"\"\"Create a single list unifying examples from all remote datasets\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize the dataset\n",
    "        self.dataset = []\n",
    "      \n",
    "        # populate the dataset list\n",
    "        for dataset in self.sets:\n",
    "            for example in dataset:\n",
    "                self.dataset.append(example)\n",
    "                \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"The collat_fn method to be used by the\n",
    "        PyTorch data loader.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Unzip the batch\n",
    "        vectors, targets = list(zip(*batch))\n",
    "\n",
    "        # concatenate the vectors\n",
    "        vectors = torch.stack(vectors)\n",
    "        \n",
    "        #concatenate the labels\n",
    "        targets = torch.stack(targets)\n",
    "        \n",
    "        return vectors, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create two such `DatasetIMDB` objects, one for training and the other for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a training Dataset object\n",
    "trainset = DatasetIMDB(sets = [train_bob,\n",
    "                               train_alice],\n",
    "                       share_workers = [bob, alice],\n",
    "                       crypto_provider = crypto_provider,\n",
    "                       nlp = nlp\n",
    "                      )\n",
    "\n",
    "# Instantiate a validation Dataset object\n",
    "valset = DatasetIMDB(sets = [val_bob,\n",
    "                             val_alice],\n",
    "                     share_workers = [bob, alice],\n",
    "                     crypto_provider = crypto_provider,\n",
    "                     nlp = nlp\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now choose some hyper parameters for training and validation, and create the PyTorch data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some hyper parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the DataLoader object for the training set\n",
    "trainloader = DataLoader(trainset, shuffle = True,\n",
    "                         batch_size = batch_size, num_workers = 0, \n",
    "                         collate_fn = trainset.collate_fn)\n",
    "\n",
    "\n",
    "# Instantiate the DataLoader object for the validation set\n",
    "valloader = DataLoader(valset, shuffle = True,\n",
    "                       batch_size = batch_size, num_workers = 0, \n",
    "                       collate_fn = valset.collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create an Encrypted Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentiment classifier I use here is a simple fully connected network with `300` input features which is the size of the embedding vectors computed by SyferText. The network has two outputs, one for negative sentiments and the other for positive ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.fc = torch.nn.Linear(in_features, out_features)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        \n",
    "        logits = self.fc(x)\n",
    "        \n",
    "        probs = F.relu(logits)\n",
    "        \n",
    "        return probs, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should now initialize and encrypt the classifier. Encryption here should of course use the same workers to hold the share and the same primitives used to encrypt the document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier(\n",
      "  (fc): Linear(in_features=300, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "classifier = Classifier(in_features = 300, out_features = 2)\n",
    "\n",
    "# Apply SMPC encryption\n",
    "classifier = classifier.fix_precision().share(bob, alice, \n",
    "                                              crypto_provider = crypto_provider,\n",
    "                                              requires_grad = True\n",
    "                                              )\n",
    "print(classifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally I create an optimizer. Notice that the optimizer does not need to be encrypted, since it operates separately within each worker holding the classifier's and embeddings' shares. We just need to make it operate on fixed precision numbers that are used to encode shares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = optim.SGD(params = classifier.parameters(),\n",
    "                  lr = learning_rate)\n",
    "\n",
    "optim = optim.fix_precision()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats!!! You are now ready to run the below cell to launch the training. \n",
    "\n",
    "Notice that we use MSE as a training loss which is not the best choice for a classification task. I choose to use it since the `NLLLoss()` is not yet implemented in PySyft for SMPC mode. But it is an issue that is currently being worked on.\n",
    "\n",
    "In order to view the training and validation curves for loss and accuracy, you need to run `Tensorboard`. Just open a terminal, navigate to the folder containing this notebook, and run:\n",
    "\n",
    "```\n",
    "$ tensorboard --logdir runs/\n",
    "```\n",
    "\n",
    "Then open you favorite web browser and go to `localhost:6006`.\n",
    "\n",
    "You should now be able to see performance curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-27272e4acc14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Set train mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# 1). Zero out previous gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    for iter, (vectors, targets) in enumerate(trainloader):\n",
    "        \n",
    "        # Set train mode\n",
    "        classifier.train()\n",
    "\n",
    "        # 1). Zero out previous gradients\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # 2). predict sentiment probabilities\n",
    "        probs, logits = classifier(vectors)\n",
    "\n",
    "        # 3). Compute loss and accuracy\n",
    "        loss = ((probs -  targets)**2).sum()\n",
    "\n",
    "\n",
    "        # Get the predicted labels\n",
    "        preds = probs.argmax(dim=1)\n",
    "        targets = targets.argmax(dim=1)\n",
    "        \n",
    "        # Compute the prediction accuracy\n",
    "        accuracy = preds.eq(targets).sum()\n",
    "        accuracy = accuracy.get().float_precision()\n",
    "        accuracy = 100 * (accuracy / batch_size)\n",
    "        \n",
    "        # 4). Backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # 5). Update weights\n",
    "        optim.step()\n",
    "\n",
    "        # Decrypt the loss for logging\n",
    "        loss = loss.get().float_precision()\n",
    "\n",
    "        \n",
    "        # Log to tensorboard\n",
    "        writer.add_scalar('train/loss', loss, epoch * len(trainloader) + iter )\n",
    "        writer.add_scalar('train/acc', accuracy, epoch * len(trainloader) + iter )\n",
    "\n",
    "        \n",
    "        \"\"\" Perform validation on exactly one batch \"\"\"\n",
    "        \n",
    "        # Set validation mode\n",
    "        classifier.eval()\n",
    "\n",
    "        for vectors, targets in valloader:\n",
    "            \n",
    "            \n",
    "            probs, logits = classifier(vectors)\n",
    "\n",
    "            loss = ((probs -  targets)**2).sum()\n",
    "\n",
    "            preds = probs.argmax(dim=1)\n",
    "            targets = targets.argmax(dim=1)\n",
    "\n",
    "            accuracy = preds.eq(targets).sum()\n",
    "            accuracy = accuracy.get().float_precision()\n",
    "            accuracy = 100 * (accuracy / batch_size)\n",
    "\n",
    "            loss = loss.get().float_precision()\n",
    "\n",
    "            # Log to tensorboard\n",
    "            writer.add_scalar('val/loss', loss, epoch * len(trainloader) + iter )\n",
    "            writer.add_scalar('val/acc', accuracy, epoch * len(trainloader) + iter )\n",
    "            \n",
    "            break\n",
    "            \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that training is finished, let me prove to you, that as I explained in **Figure(2)**, both Bob and Alice has `SubPipeline` objects on their machines sent by SyferText that contain the pipeline components I defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SubPipeline[tokenizer], SubPipeline[tokenizer > stop tagger]]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On bob's machine\n",
    "[bob._objects[id] for id in bob._objects if  isinstance(bob._objects[id], syfertext.SubPipeline)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SubPipeline[tokenizer > stop tagger]]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On Alices's machine\n",
    "[alice._objects[id] for id in alice._objects if  isinstance(alice._objects[id], syfertext.SubPipeline)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
